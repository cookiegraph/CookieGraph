import pandas as pd
from .cookies import *
import pymysql
import sqlite3
import ast


def read_tables(conn, visit_id):

    df_http_requests = pd.read_sql_query("SELECT visit_id, request_id, "
                                         "url, headers, top_level_url, resource_type, "
                                         f"time_stamp, post_body, post_body_raw from http_requests where {visit_id} = visit_id", conn)
    df_http_responses = pd.read_sql_query("SELECT visit_id, request_id, "
                                          "url, headers, response_status, time_stamp, content_hash "
                                          f" from http_responses where {visit_id} = visit_id", conn)
    df_http_redirects = pd.read_sql_query("SELECT visit_id, old_request_id, "
                                          "old_request_url, new_request_url, response_status, "
                                          f"headers, time_stamp from http_redirects where {visit_id} = visit_id", conn)
    call_stacks = pd.read_sql_query(
        f"SELECT visit_id, request_id, call_stack from callstacks where {visit_id} = visit_id", conn)
    javascript = pd.read_sql_query("SELECT visit_id, script_url, script_line, script_loc_eval, top_level_url, document_url, symbol, call_stack, operation,"
                                   f" arguments, attributes, value, time_stamp from javascript where {visit_id} = visit_id", conn)
    return df_http_requests, df_http_responses, df_http_redirects, call_stacks, javascript

def get_sites_visit(conn):

    #df_successful_sites = pd.read_sql_query("SELECT visit_id from crawl_history where "
    #                                        "command = 'GetCommand' and command_status = 'ok'", conn)

    df_successful_sites = pd.read_sql_query("SELECT visit_id from crawl_history where "
                                            "command = 'GetCommand'", conn)
    
    successful_vids = df_successful_sites['visit_id'].tolist()
    print(successful_vids)
    query = "SELECT visit_id, site_url from site_visits where visit_id in %s" % str(tuple(successful_vids))
    
    return pd.read_sql_query(query, conn)

def get_cookies_info(conn, visit_id):

    df_js_cookie = pd.read_sql("select visit_id, time_stamp, script_url, "
                               "document_url, top_level_url, call_stack, operation, "
                               "value from javascript where symbol='window.document.cookie'", conn, parse_dates=['time_stamp'])

    return df_js_cookie

def get_local_db_connection(db_file):

    conn = sqlite3.connect(db_file)
    return conn


def get_remote_db_connection(host_name, user_name, password, ssl_ca, ssl_key, ssl_cert, database_name, port):
    return pymysql.connect(host=host_name, port=port, user=user_name, password=password,
                           database=database_name, ssl_ca=ssl_ca, ssl_key=ssl_key, ssl_cert=ssl_cert)

# Method to read the sqlite file generated by the scraper
def read_scraper_data(db_file):
    dict_scraper = {}
    with  sqlite3.connect(db_file) as con:
        cur = con.cursor()
        for row in cur.execute('SELECT * FROM posts;'):
            title = row[0].strip()
            # Top Level URL
            top_level_url_scraper = ast.literal_eval(row[4])
            # I may need to retrieve more information here when I parse multiple crawls
            filter_ = row[6].strip()
            dict_scraper[title] = (top_level_url_scraper,filter_)
    return dict_scraper
